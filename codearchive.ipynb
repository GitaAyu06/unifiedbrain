{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCRN_gcn(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim, hidden_dim, num_layers=2):\n",
    "        super(GCRN_gcn, self).__init__()\n",
    "        self.encoder = GCNEncoder(input_dim, feature_dim)\n",
    "        self.decoder = GCNDecoder(hidden_dim, input_dim)  # Assuming input_dim is the number of nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Create a vanilla GRU layer with the specified number of layers\n",
    "        self.gru = nn.GRU(feature_dim, hidden_dim, num_layers)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        num_nodes, feature_dim = x.size()\n",
    "        \n",
    "        # Encode the current time point\n",
    "        z, mu, logvar = self.encoder(x, adj)\n",
    "        z = z.unsqueeze(0).unsqueeze(0)  # (1, 1, feature_dim)\n",
    "        \n",
    "        # Initialize hidden states for GRU\n",
    "        h = torch.zeros(self.num_layers, 1, self.hidden_dim, device=x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        # Update latent representation using the GRU\n",
    "        z, h = self.gru(z, h)\n",
    "        z = z.squeeze(0).squeeze(0) # (hidden_dim,)\n",
    "        \n",
    "        # Decode the updated latent representation to predict the adjacency matrix for the next time point\n",
    "        adj_pred = self.decoder(z)\n",
    "        \n",
    "        return adj_pred, mu, logvar\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "def loss_function(recon_adj, adj, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon_adj, adj, reduction='sum')\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld_loss\n",
    "\n",
    "\n",
    "def train_model(model, train_features, train_adj, num_epochs=100, lr=0.001, save_path='gcrn_model.pth'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    training_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(train_features.size(0)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Extract current and next time point data\n",
    "            x_t = train_features[i, 0]  # Current time point node features\n",
    "            adj_t = train_adj[i, 0]     # Current time point adjacency matrix\n",
    "            adj_t_next = train_adj[i, 1]  # Next time point adjacency matrix (ground truth)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_adj, mu, logvar = model(x_t, adj_t)\n",
    "            # Compute loss\n",
    "            loss = loss_function(recon_adj, adj_t_next, mu, logvar)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= train_features.size(0)\n",
    "        training_loss.append(epoch_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n",
    "\n",
    "    # Plot the training loss\n",
    "    plt.plot(training_loss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved to {save_path}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
