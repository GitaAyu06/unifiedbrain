{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = '/Users/gitaayusalsabila/Documents/0thesis/code/sandbox/dataset/'\n",
    "dataset_path = '/notebooks/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_simulate(datapath, n_samples=200,n_time=2,n_views=4):\n",
    "    # Note that changing the node count is not provided right now, since we use correlation matrix\n",
    "    # and the mean values of connectivities from real data and it is for 35 nodes.\n",
    "    \n",
    "    # Import all required statistical information.\n",
    "    allstats = np.load(datapath + \"stats/REAL_TIME_DIFF.npy\") # Connectivity mean values of LH. You can also try with RH.\n",
    "    allcorrs = np.load(datapath + \"stats/REAL_TIME_DIFF.npy\") # Correlation matrix in LH. You can also try with RH.\n",
    "    all_diffs = np.load(datapath + \"stats/REAL_TIME_DIFF.npy\") # This is an overall representation of time differences in both (LH and RH) datasets.\n",
    "    \n",
    "    times = []\n",
    "    for t in range(n_time):\n",
    "        views = []\n",
    "        for v in range(n_views):\n",
    "            # Note that we randomly assign a new random state to ensure it will generate a different dataset at each run.\n",
    "            # Generate data with the correlations and mean values at the current timepoint.\n",
    "            if t < 2:\n",
    "                connectomic_means = allstats[t,v]\n",
    "                data = multivariate_normal.rvs(connectomic_means,allcorrs[t,v],n_samples,random_state=randint(1,9999))\n",
    "            # If the requested timepoints are more than we have in real data, use the correlation information from the last timepoint.\n",
    "            else:\n",
    "                connectomic_means = allstats[-1,v]\n",
    "                data = multivariate_normal.rvs(connectomic_means,allcorrs[-1,v],n_samples,random_state=randint(1,9999))\n",
    "\n",
    "            adj = []\n",
    "            for idx, sample in enumerate(data):\n",
    "                # Create adjacency matrix.\n",
    "                matrix = antiVectorize(sample,35)\n",
    "                # Perturb the real time difference with nonlinear tanh function.\n",
    "                noise = np.tanh( t / n_time )\n",
    "                # Further timepoints will have more significant difference from the baseline (t=6 >> t=1).\n",
    "                matrix = matrix + all_diffs[:,:,v] * ( noise + 1 )\n",
    "                adj.append(matrix)\n",
    "            views.append(np.array(adj))\n",
    "\n",
    "        times.append(np.array(views))\n",
    "    \n",
    "    alldata=np.array(times)\n",
    "    alldata = np.transpose(alldata,(2,0,3,4,1))\n",
    "    return alldata \n",
    "\n",
    "def prepare_data(datapath, new_data=False, n_samples=200, n_times=6):\n",
    "    # Note that data with 200 samples and 6 timepoints is very large (5.8M data points),\n",
    "    # check your GPU memory to make sure there is enough space to allocate. If not, try:\n",
    "    # - to reduce dataset size by changing n_samples or n_times.\n",
    "    # - on CPU (this will allocate memory on RAM) --> This might work for example if you have 1GB GPU memory but 16GB RAM.\n",
    "    # - on another computer with a better NVIDIA graphics card. --> 2GB GPU memory will not be enough for 5.8M data.\n",
    "    try:\n",
    "        if new_data:\n",
    "            samples = multivariate_simulate(datapath, n_samples, n_times)\n",
    "            np.save(datapath + 'simulated_adj.npy',samples)\n",
    "        else:\n",
    "            samples = np.load(datapath + 'simulated_adj.npy')\n",
    "    except:\n",
    "        samples = multivariate_simulate(datapath, n_samples, n_times)\n",
    "        np.save(datapath + 'simulated_adj.npy',samples)\n",
    "    return samples\n",
    "\n",
    "simulated_data = prepare_data(dataset_path, new_data=False, n_samples=100, n_times=3)\n",
    "simulated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim160 = np.load(dataset_path + 'slim160_adj.npy')\n",
    "print(slim160.shape)\n",
    "\n",
    "slim268 = np.load(dataset_path + 'slim268_adj.npy')\n",
    "print(slim268.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing(dataset):\n",
    "    # Replace negative values with 0\n",
    "    dataset[dataset < 0] = 0\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    dataset = np.nan_to_num(dataset, nan=0)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def check_and_drop_invalid_graphs(graph_dataset):\n",
    "    \"\"\"\n",
    "    Check that all graphs in the dataset have more than 0 edges and drop graphs with 0 edges in any timepoint or dimension.\n",
    "    \n",
    "    Parameters:\n",
    "    graph_dataset (np.ndarray): The input graph dataset with shape [g, t, n, n, d] or [g, t, n, n]\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The cleaned dataset with invalid graphs removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check the shape of the dataset to determine if it has multiple dimensions\n",
    "    if len(graph_dataset.shape) == 5:\n",
    "        num_graphs, num_timepoints, num_nodes, _, num_dimensions = graph_dataset.shape\n",
    "    else:\n",
    "        num_graphs, num_timepoints, num_nodes, _ = graph_dataset.shape\n",
    "        num_dimensions = 1\n",
    "    \n",
    "    valid_graphs = []\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        is_valid = True\n",
    "        for t in range(num_timepoints):\n",
    "            for d in range(num_dimensions):\n",
    "                if num_dimensions > 1:\n",
    "                    adj_matrix = graph_dataset[i, t, :, :, d]\n",
    "                else:\n",
    "                    adj_matrix = graph_dataset[i, t, :, :]\n",
    "                \n",
    "                num_edges = np.sum(adj_matrix > 0)\n",
    "                if num_edges == 0:\n",
    "                    is_valid = False\n",
    "                    break\n",
    "            if not is_valid:\n",
    "                break\n",
    "        \n",
    "        if is_valid:\n",
    "            valid_graphs.append(i)\n",
    "    \n",
    "    if num_dimensions > 1:\n",
    "        cleaned_dataset = graph_dataset[valid_graphs, :, :, :, :]\n",
    "    else:\n",
    "        cleaned_dataset = graph_dataset[valid_graphs, :, :, :]\n",
    "    \n",
    "    return cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_cleaned = data_cleansing(simulated_data)\n",
    "simulated_cleaned = check_and_drop_invalid_graphs(simulated_data)\n",
    "print('simulated data shape:',simulated_cleaned.shape)\n",
    "\n",
    "slim160_cleaned = data_cleansing(slim160)\n",
    "slim160_cleaned = check_and_drop_invalid_graphs(slim160_cleaned)\n",
    "print('slim160 shape:',slim160_cleaned.shape)\n",
    "\n",
    "slim268_cleaned = data_cleansing(slim268)\n",
    "slim268_cleaned = check_and_drop_invalid_graphs(slim268_cleaned)\n",
    "print('slim268 shape:',slim268_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laplacian Encoding\n",
    "simulated_laplacian_features = np.load(dataset_path + 'simulated_laplacian_features.npy') \n",
    "slim160_laplacian_features = np.load(dataset_path + 'slim160_laplacian_features.npy') \n",
    "slim268_laplacian_features = np.load(dataset_path + 'slim268_laplacian_features.npy') \n",
    "\n",
    "## Degree Encoding\n",
    "simulated_degree_features = np.load(dataset_path + 'simulated_degree_features.npy') \n",
    "slim160_degree_features = np.load(dataset_path + 'slim160_degree_features.npy') \n",
    "slim268_degree_features = np.load(dataset_path + 'slim268_degree_features.npy') \n",
    "\n",
    "## Node2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucledian_distance(x):\n",
    "  repeated_out = x.repeat(35,1,1)\n",
    "  repeated_t = torch.transpose(repeated_out, 0, 1)\n",
    "  diff = torch.abs(repeated_out - repeated_t)\n",
    "  return torch.sum(diff, 2)\n",
    "\n",
    "def frobenious_distance(test_sample,predicted):\n",
    "  diff = torch.abs(test_sample - predicted)\n",
    "  dif = diff*diff\n",
    "  sum_of_all = diff.sum()\n",
    "  d = torch.sqrt(sum_of_all)\n",
    "  return d\n",
    "\n",
    "def create_edge_index_attribute(adj_matrix):\n",
    "    rows, cols = adj_matrix.shape[0], adj_matrix.shape[1]\n",
    "    edge_index = torch.zeros((2, rows * cols), dtype=torch.long, device='cuda')\n",
    "    edge_attr = torch.zeros((rows * cols, 1), dtype=torch.float, device='cuda')\n",
    "    counter = 0\n",
    "\n",
    "    for src, attrs in enumerate(adj_matrix):\n",
    "        for dest, attr in enumerate(attrs):\n",
    "            edge_index[0][counter], edge_index[1][counter] = src, dest\n",
    "            edge_attr[counter] = attr\n",
    "            counter += 1\n",
    "\n",
    "    return edge_index, edge_attr, rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.weight = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.weight_h = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.out = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.tanh = Tanh()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        global hidden_state\n",
    "        h = hidden_state\n",
    "        y = self.tanh(self.weight(x) + self.weight_h(h))\n",
    "        hidden_state = y.detach()\n",
    "        return y\n",
    "\n",
    "\n",
    "class RBGM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RBGM, self).__init__()\n",
    "        self.rnn = nn.Sequential(RNNCell(1,1225), ReLU())\n",
    "        self.gnn_conv = NNConv(35, 35, self.rnn, aggr='mean', root_weight=True, bias = True)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        edge_index, edge_attr, _, _ = create_edge_index_attribute(data)\n",
    "        x1 = F.relu(self.gnn_conv(data, edge_index, edge_attr))\n",
    "        x1 = eucledian_distance(x1)\n",
    "        return x1\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbgm_simulated_dom1 = GCRN(input_dim, feature_dim, latent_dim, encoder_hidden_dims, decoder_hidden_dims, num_rec_layers, 'GAT', 'GCN')\n",
    "\n",
    "print(rbgm_simulated_dom1)\n",
    "print(f\"Total number of trainable parameters: {(rbgm_simulated_dom1.count_parameters())*2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mael = torch.nn.L1Loss().to(device)\n",
    "tp = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rbgm(model, train_adj, num_epochs=100, lr=0.001, save_path='models/rgbm_model.pth'):\n",
    "                    \n",
    "    model_1 = rbgm_simulated_dom1\n",
    "    model_2 = rbgm_simulated_dom1\n",
    "    optimizer_1 = torch.optim.Adam(model_1.parameters(), lr = lr)\n",
    "    optimizer_2 = torch.optim.Adam(model_2.parameters(), lr = lr)\n",
    "\n",
    "    training_loss = []\n",
    "\n",
    "    epoch_time = []\n",
    "    cpu_usage = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    images = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        set_seed(42)\n",
    "        # Will be used for reporting\n",
    "\n",
    "        train_loss_t1, train_loss_t2 = 0.0, 0.0 \n",
    "        mae_loss_eval_t1, mae_loss_eval_t2 = 0.0, 0.0\n",
    "        tp_loss_1, tp_loss_2, tr_loss_1, tr_loss_2 = 0.0, 0.0, 0.0, 0.0\n",
    "        model_1.train()\n",
    "        model_2.train()\n",
    "        for i, data in enumerate(h_data_train_loader):\n",
    "            \n",
    "            \n",
    "            #Time Point 1\n",
    "            data = data.to(device)\n",
    "            optimizer_1.zero_grad()\n",
    "            out_1 = model_1(data.x)\n",
    "            \n",
    "           \n",
    "            # Topological Loss\n",
    "            \n",
    "            tp_1 = tp(out_1.sum(dim=-1), data.y.sum(dim=-1))\n",
    "            tp_loss_1 += tp_1.item()\n",
    "            \n",
    "            #MAE Loss\n",
    "            loss_1 = mael(out_1, data.y)\n",
    "            train_loss_t1 += loss_1.item()\n",
    "            \n",
    "            total_loss_1 = loss_1 + opt.tp_coef * tp_1\n",
    "            tr_loss_1 += total_loss_1.item()\n",
    "            total_loss_1.backward()\n",
    "            optimizer_1.step()\n",
    "            \n",
    "            #Time Point 2\n",
    "            \n",
    "            optimizer_2.zero_grad()\n",
    "            out_2 = model_2(data.y)\n",
    "            \n",
    "\n",
    "            # Topological Loss\n",
    "           \n",
    "            tp_2 = tp(out_2.sum(dim=-1), data.y2.sum(dim=-1))\n",
    "            tp_loss_2 += tp_2.item()\n",
    "            \n",
    "            \n",
    "            #MAE Loss\n",
    "            loss_2 = mael(out_2, data.y2)\n",
    "            train_loss_t2 += loss_2.item()\n",
    "            \n",
    "            total_loss_2 = loss_2 + opt.tp_coef * tp_2\n",
    "            tr_loss_2 += total_loss_2.item()\n",
    "            total_loss_2.backward()\n",
    "            optimizer_2.step()\n",
    "            \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "        print(f'[Train] Loss T1 : {train_loss_t1 / total_step:.5f}, Loss T2 : {train_loss_t2 / total_step:.5f} ')\n",
    "        print(f'[Train] TP Loss T1: {tp_loss_1 / total_step:.5f}, TP Loss T2 : {tp_loss_2 / total_step:.5f}')\n",
    "        print(f'[Train] Total Loss T1: {tr_loss_1 / total_step:.5f}, Total Loss T2 : {tr_loss_2 / total_step:.5f}' )\n",
    "        mae_loss_train_t1.append(train_loss_t1 / total_step)\n",
    "        mae_loss_train_t2.append(train_loss_t2 / total_step)\n",
    "        tp_loss_train_1.append(tp_loss_1 / total_step)\n",
    "        tp_loss_train_2.append(tp_loss_2 / total_step)\n",
    "        train_total_loss_1.append(tr_loss_1 / total_step)\n",
    "        train_total_loss_2.append(tr_loss_2 / total_step)\n",
    "        \n",
    "    \n",
    "    tic1 = timeit.default_timer()\n",
    "    timer(tic0,tic1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EvoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
